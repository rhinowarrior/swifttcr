"""
Name: calculate_rmsd.py
Function: Calculate LRMSD, IRMSD, and FNAT between reference and decoy files.
Date: 16-10-2024
Author: Jan Aarts, Farzaneh Meimandi Parizi, Nils Smit
"""

"""
Input:
    - mapped_dir: Directory containing the mapped files
    - original_dir: Directory with original reference models
    - cluster_input: Name of the clustering file
    - outfile_base: Base name for the output files
    - num_processes: Number of processes to use for multiprocessing

Example: python calculate_rmsd.py mapped_dir original_dir cluster_input outfile_base num_processes
"""
from pathlib import Path
import os
from sys import argv
from pdb2sql.StructureSimilarity import StructureSimilarity
from multiprocessing import Pool
import gc

def add_suffix(path, suffix):
    """Adds a suffix to the filename and returns the new Path object.
    
    Args:
        path (Path): The original Path object.
        suffix (str): The suffix to add to the filename.
    
    Returns:
        Path: The new Path object with the added suffix.
    """
    before = path.parent
    after = path.stem
    return Path(before, f"{after}_{suffix}{path.suffix}")

def parse_clustering_file(clustering_path):
    """Parses a clustering file and extracts model names.
    
    Args:
        clustering_path (str): Path to the clustering file.
    
    Returns:
        list: List of model names extracted from the clustering file.
    """
    models = []
    with open(clustering_path, 'r') as f:
        lines = f.readlines()
    for line in lines:
        if line.startswith("Cluster center: "):
            model_name = line.split("Cluster center: ")[-1]
            model_name = model_name.split("with")[0].strip()
            cleaned_name = model_name.replace("merged_", "").replace(".pdb", "").strip()
            models.append(cleaned_name)
    return models

def search_for_clustering_files(base_dir, cluster_input):
    """Search for clustering files and create a dictionary of models.
    
    Args:
        base_dir (str): Base directory to search for clustering files.
        cluster_input (str): Name of the clustering file.
    
    Returns:
        dict: Dictionary containing the model names and their clustering files.
    """
    model_dict = {}
    base_path = Path(base_dir)
    
    for model_dir in base_path.glob('**/'):
        cluster_file_path = model_dir / cluster_input
        if cluster_file_path.exists():
            key = model_dir.stem[:4]
            models = parse_clustering_file(cluster_file_path)
            model_dict[key] = models
    
    return model_dict

def calc_LRMSD_decoys(ref_file, decoy_files, thread_num):
    """Calculates LRMSD, IRMSD, and FNAT between reference and decoy files.
    
    Args:
        ref_file (str): Path to the reference PDB file.
        decoy_files (list): List of paths to the decoy PDB files.
        thread_num (int): Thread number for unique Lzone file.
    
    Returns:
        tuple: Tuple containing lists of LRMSD, IRMSD, and FNAT values.
    """
    lrmsds, irmsds, fnats = [], [], []

    lzone_file = f'ref_lzone_{thread_num}.lzone'
    
    # Remove the Lzone file if it already exists
    if os.path.exists(lzone_file):
        os.remove(lzone_file)
    
    for decoy_file in decoy_files:
        try:
            # Create a StructureSimilarity object for the reference and decoy files
            sim = StructureSimilarity(ref_file, decoy_file, enforce_residue_matching=False)

            # Calculate LRMSD, IRMSD, and FNAT values. The Lzone file is automaticlly generated by the pdb2sql package. it is created by aligning the longest chain of the decoy to the one of the reference and computing the RMSD of the shortest chain between decoy and reference.
            lrmsd_pdb2sql = sim.compute_lrmsd_fast(lzone=lzone_file)
            irmsd_pdb2sql = sim.compute_irmsd_fast()
            fnat_pdb2sql = sim.compute_fnat_fast()

            lrmsds.append(lrmsd_pdb2sql)
            irmsds.append(irmsd_pdb2sql)
            fnats.append(fnat_pdb2sql)

            print(f"Process {thread_num}: {os.path.basename(decoy_file)} - LRMSD: {lrmsd_pdb2sql}, IRMSD: {irmsd_pdb2sql}, FNAT: {fnat_pdb2sql}")

        except Exception as e:
            print(f"Error in {os.path.basename(decoy_file)}: {str(e)}")
            
        finally:
            # Clean up the StructureSimilarity object
            del sim
            gc.collect()

    # Clean up the unique Lzone file after processing
    if os.path.exists(lzone_file):
        os.remove(lzone_file)

    return lrmsds, irmsds, fnats

def process_model(args):
    """Processes a single model and calculates RMSD values.
    
    Args:
        args: Tuple with the model identifier, file number, and mapped directory.
    
    Returns:
        Tuple with the results of the RMSD calculations.
    """
    index, model_identifier, file_number, mapped_dir, thread_number = args
    ref_file = mapped_dir / f"{model_identifier}_reference_{file_number}_mapped.pdb"
    decoy_file = mapped_dir / f"{model_identifier}_merged_{file_number}_mapped.pdb"

    # Check if the reference and decoy files exist
    if ref_file.exists() and decoy_file.exists():
        print(f"Processing {ref_file.name} and {decoy_file.name}")
        lrmsd_values, irmsd_values, fnat_values = calc_LRMSD_decoys(ref_file, [decoy_file], thread_number)
        return index, model_identifier, file_number, lrmsd_values, irmsd_values, fnat_values
    else:
        print(f"Files do not exist: {ref_file} or {decoy_file}")
        return index, model_identifier, file_number, [], [], []  # Empty results for missing files

def set_thread_limit(num_threads):
    """
    Sets thread limit for multi-threaded libraries like NumPy, MKL, OpenMP.
    
    Args:
        num_threads (int): Number of threads to use.
    """
    os.environ["OMP_NUM_THREADS"] = str(num_threads)
    os.environ["MKL_NUM_THREADS"] = str(num_threads)
    os.environ["NUMEXPR_NUM_THREADS"] = str(num_threads)
    os.environ["OPENBLAS_NUM_THREADS"] = str(num_threads)

def main():
    mapped_dir = Path(argv[1])     # Directory containing the mapped files
    original_dir = Path(argv[2])   # Directory with original reference models
    cluster_input = argv[3]        # Name of the clustering file
    outfile_base = Path(argv[4])   # Convert to Path object
    num_processes = int(argv[5])   # Number of processes specified by the user
    
    # Set thread limit for multi-threaded libraries
    set_thread_limit(1)

    model_dict = search_for_clustering_files(original_dir, cluster_input)

    # Prepare arguments for each model processing task
    args_list = []
    index = 0  # This will track the order of the tasks
    
    for model_identifier, file_numbers in model_dict.items():
        for file_number in file_numbers:
            thread_number = (index % num_processes) + 1  # Cycle thread numbers from 1 to num_processes
            args_list.append((index, model_identifier, file_number, mapped_dir, thread_number))
            index += 1  # Increment index for each task

    # Use a multiprocessing Pool to process models
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_model, args_list)

    # Sort results by their original index to ensure correct order
    results.sort(key=lambda x: x[0])

    # Separate results into dictionaries for LRMSD, IRMSD, and FNAT
    results_lrmsd = {}
    results_irmsd = {}
    results_fnat = {}

    # Collect results for each model identifier
    for index, model_identifier, file_number, lrmsd_values, irmsd_values, fnat_values in results:
        if model_identifier not in results_lrmsd:
            results_lrmsd[model_identifier] = []
        results_lrmsd[model_identifier].extend(lrmsd_values)

        if model_identifier not in results_irmsd:
            results_irmsd[model_identifier] = []
        results_irmsd[model_identifier].extend(irmsd_values)

        if model_identifier not in results_fnat:
            results_fnat[model_identifier] = []
        results_fnat[model_identifier].extend(fnat_values)

    # Write results to separate output files
    lrmsd_outfile = add_suffix(outfile_base, "lrmsd")
    irmsd_outfile = add_suffix(outfile_base, "irmsd")
    fnat_outfile = add_suffix(outfile_base, "fnat")

    with open(lrmsd_outfile, 'w') as f_lrmsd, \
         open(irmsd_outfile, 'w') as f_irmsd, \
         open(fnat_outfile, 'w') as f_fnat:

        for model_id, lrmsd_values in results_lrmsd.items():
            f_lrmsd.write(f"{model_id}\t" + "\t".join(f"{val}" for val in lrmsd_values) + "\n")

        for model_id, irmsd_values in results_irmsd.items():
            f_irmsd.write(f"{model_id}\t" + "\t".join(f"{val}" for val in irmsd_values) + "\n")

        for model_id, fnat_values in results_fnat.items():
            f_fnat.write(f"{model_id}\t" + "\t".join(f"{val}" for val in fnat_values) + "\n")

    print("Results written to", lrmsd_outfile, irmsd_outfile, fnat_outfile)

if __name__ == "__main__":
    main()
